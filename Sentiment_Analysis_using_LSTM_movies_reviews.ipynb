{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nhung-Nguyen86/DataScience/blob/main/Sentiment_Analysis_using_LSTM_movies_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAbBA21qjNya"
      },
      "source": [
        "# Sentiment Analysis Using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpw9cTvNjNyk"
      },
      "source": [
        "In this assignment you will use an RNN/LSTM model for sentiment Analysis. You will use the movie reviews from the UCI Sentiment Labelled Sentences Dataset: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIrd5sWwjNyo"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkvyR-4EjNys"
      },
      "source": [
        "### Load the data\n",
        "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxlNQkYljNyu",
        "outputId": "7ecb39bb-7269-423f-fa0d-e0e7a0822eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000 train sequences\n",
            "25000 test sequences\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\geoec\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "c:\\Users\\geoec\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ]
        }
      ],
      "source": [
        "max_features = 20000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_iY0AibjNyw"
      },
      "source": [
        "#### Pad the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmVKHoBEjNyx"
      },
      "outputs": [],
      "source": [
        "maxlen = 80\n",
        "batch_size = 32\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYRqo1qrjNyz"
      },
      "source": [
        "### Define the model\n",
        "define an LSTM model with an emebeding layer followed(to learn the embeddings), followed by 128 layers of LSTM units followed by aflatten, followed by two dense fully connceted layers 200 and 100 units, and finally a sigmoid layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i7yhL8gjNy0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=20000, output_dim=128, input_length=80))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCSi_33UjNy1"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoAk4ZF-jNy2",
        "outputId": "57f507d5-945d-4fea-b7b5-4d159e0ef0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "782/782 [==============================] - 29s 24ms/step - loss: 0.4303 - accuracy: 0.7953 - val_loss: 0.3614 - val_accuracy: 0.8403\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.2398 - accuracy: 0.9046 - val_loss: 0.3756 - val_accuracy: 0.8394\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.1396 - accuracy: 0.9484 - val_loss: 0.4463 - val_accuracy: 0.8194\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.0877 - accuracy: 0.9676 - val_loss: 0.4891 - val_accuracy: 0.8161\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.0547 - accuracy: 0.9806 - val_loss: 0.6967 - val_accuracy: 0.8259\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.0386 - accuracy: 0.9869 - val_loss: 0.8795 - val_accuracy: 0.8207\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 30s 39ms/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 0.8610 - val_accuracy: 0.8058\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.8487 - val_accuracy: 0.8141\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 46s 59ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 1.0216 - val_accuracy: 0.8234\n",
            "Epoch 10/15\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.8394 - val_accuracy: 0.8084\n",
            "Epoch 11/15\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 1.1313 - val_accuracy: 0.8214\n",
            "Epoch 12/15\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 1.5299 - val_accuracy: 0.8228\n",
            "Epoch 13/15\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 1.2241 - val_accuracy: 0.8247\n",
            "Epoch 14/15\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 1.3010 - val_accuracy: 0.8134\n",
            "Epoch 15/15\n",
            "782/782 [==============================] - 18s 22ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 1.1371 - val_accuracy: 0.8208\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1d8f4ee5700>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiQUy3qljNy2",
        "outputId": "c2edfe47-1545-41f3-e095-c61974c726ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 1.1371 - accuracy: 0.8208\n",
            "Test score: 1.1370962858200073\n",
            "Test accuracy: 0.8208400011062622\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKeSo9JxjNy3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}